{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('/Users/seunghoonchoi/Downloads/Dacon_recommend_system')\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "from dgl.nn import HeteroGraphConv, SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {'SEED' : 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall5(answer_df, submission_df):\n",
    "    \"\"\"\n",
    "    Calculate recall@5 for given dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    - answer_df: DataFrame containing the ground truth\n",
    "    - submission_df: DataFrame containing the predictions\n",
    "    \n",
    "    Returns:\n",
    "    - recall: Recall@5 value\n",
    "    \"\"\"\n",
    "    \n",
    "    primary_col = answer_df.columns[0]\n",
    "    secondary_col = answer_df.columns[1]\n",
    "    \n",
    "    # Check if each primary_col entry has exactly 5 secondary_col predictions\n",
    "    prediction_counts = submission_df.groupby(primary_col).size()\n",
    "    if not all(prediction_counts == 5):\n",
    "        raise ValueError(f\"Each {primary_col} should have exactly 5 {secondary_col} predictions.\")\n",
    "\n",
    "\n",
    "    # Check for NULL values in the predicted secondary_col\n",
    "    if submission_df[secondary_col].isnull().any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains NULL values.\")\n",
    "    \n",
    "    # Check for duplicates in the predicted secondary_col for each primary_col\n",
    "    duplicated_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].duplicated().any())\n",
    "    if duplicated_preds.any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains duplicates for some {primary_col}.\")\n",
    "\n",
    "\n",
    "    # Filter the submission dataframe based on the primary_col present in the answer dataframe\n",
    "    submission_df = submission_df[submission_df[primary_col].isin(answer_df[primary_col])]\n",
    "    \n",
    "    # For each primary_col, get the top 5 predicted secondary_col values\n",
    "    top_5_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].head(5).tolist()).to_dict()\n",
    "    \n",
    "    # Convert the answer_df to a dictionary for easier lookup\n",
    "    true_dict = answer_df.groupby(primary_col).apply(lambda x: x[secondary_col].tolist()).to_dict()\n",
    "    \n",
    "    \n",
    "    individual_recalls = []\n",
    "    for key, val in true_dict.items():\n",
    "        if key in top_5_preds:\n",
    "            correct_matches = len(set(true_dict[key]) & set(top_5_preds[key]))\n",
    "            individual_recall = correct_matches / min(len(val), 5) # 공정한 평가를 가능하게 위하여 분모(k)를 'min(len(val), 5)' 로 설정함 \n",
    "            individual_recalls.append(individual_recall)\n",
    "\n",
    "\n",
    "    recall = np.mean(individual_recalls)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이력서 관련\n",
    "resume = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/resume.csv')\n",
    "resume_edu = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/resume_education.csv')\n",
    "resume_cert = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/resume_certificate.csv')\n",
    "resume_lang = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/resume_language.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공고 관련\n",
    "recruitment = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/recruitment.csv')\n",
    "company = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/company.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이력서 - 공고 매칭\n",
    "apply_train = pd.read_csv('/Users/seunghoonchoi/Downloads/Dacon_recommend_system/Data/apply_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_edu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_cert.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_lang.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_lang['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_lang['exam_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume를 제외한 이력서 관련 데이터프레임에 'resume_seq' 중복값 있는지 확인\n",
    "resume_edu_duplicates = resume_edu['resume_seq'].value_counts()\n",
    "resume_cert_duplicates = resume_cert['resume_seq'].value_counts()\n",
    "resume_lang_duplicates = resume_lang['resume_seq'].value_counts()\n",
    "\n",
    "resume_edu_duplicates_count = resume_edu_duplicates[resume_edu_duplicates > 1].count()\n",
    "resume_cert_duplicates_count = resume_cert_duplicates[resume_cert_duplicates > 1].count()\n",
    "resume_lang_duplicates_count = resume_lang_duplicates[resume_lang_duplicates > 1].count()\n",
    "\n",
    "resume_edu_duplicates_count, resume_cert_duplicates_count, resume_lang_duplicates_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자격증 종류 확인\n",
    "unique_certificate_contents = resume_cert['certificate_contents'].nunique()\n",
    "unique_certificate_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많은 자격증을 가진 사람이 몇 개의 자격증을 가졌는지.\n",
    "max_certificates = resume_cert['resume_seq'].value_counts().max()\n",
    "max_certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_seq별로 자격증 그룹화 및 개수 반영(null값 제거)\n",
    "resume_cert_cleaned = resume_cert.dropna(subset=['certificate_contents'])\n",
    "\n",
    "cert_grouped = resume_cert_cleaned.groupby('resume_seq').agg(\n",
    "    certificate_list=('certificate_contents', list),\n",
    "    certificate_count=('certificate_contents', 'size')\n",
    ").reset_index()\n",
    "\n",
    "all_resume_seq = resume[['resume_seq']]\n",
    "cert_grouped_complete = all_resume_seq.merge(cert_grouped, on='resume_seq', how='left')\n",
    "cert_grouped_complete['certificate_list'] = cert_grouped_complete['certificate_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "cert_grouped_complete['certificate_count'].fillna(0, inplace=True)\n",
    "cert_grouped_complete['certificate_count'] = cert_grouped_complete['certificate_count'].astype(int)\n",
    "\n",
    "cert_grouped_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외국어 종류 및 자격 개수만 원핫 인코딩, 점수나 시험 종류는 드랍.\n",
    "lang_encoded = pd.get_dummies(resume_lang, columns=['language'], prefix=\"\", prefix_sep=\"\")\n",
    "lang_grouped = lang_encoded.groupby('resume_seq').sum().reset_index()\n",
    "lang_grouped_cleaned = lang_grouped.drop(columns=['exam_name', 'score'])\n",
    "lang_grouped_cleaned.columns = ['resume_seq', 'lang_2', 'lang_3', 'lang_4', 'lang_8', 'lang_9']\n",
    "lang_grouped_cleaned[['lang_2', 'lang_3', 'lang_4', 'lang_8', 'lang_9']] = lang_grouped_cleaned[['lang_2', 'lang_3', 'lang_4', 'lang_8', 'lang_9']].clip(upper=1)\n",
    "lang_grouped_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 없앤 이력서 관련 데이터프레임들 병합\n",
    "final_resume = resume.merge(cert_grouped_complete, on='resume_seq', how='left')\n",
    "final_resume = final_resume.merge(lang_grouped_cleaned, on='resume_seq', how='left')\n",
    "final_resume = final_resume.merge(resume_edu, on='resume_seq', how='left')\n",
    "\n",
    "final_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitment['address_seq1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공고 주소가 빈 데이터 확인\n",
    "recruitment[recruitment['address_seq1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저 공고의 회사가 어떤 주소인지 확인하기 위해, 저 공고에 지원한 이력서들이 다른 어디에 지원했는지 확인\n",
    "applied_resumes_check = apply_train[apply_train['recruitment_seq'] == 'R01512']\n",
    "resume_seqs_for_R01512 = applied_resumes_check['resume_seq'].tolist()\n",
    "other_applications = apply_train[apply_train['resume_seq'].isin(resume_seqs_for_R01512)]\n",
    "other_recruitments_seq = other_applications['recruitment_seq'].unique()\n",
    "\n",
    "addresses_for_other_recruitments = recruitment[recruitment['recruitment_seq'].isin(other_recruitments_seq)]['address_seq1']\n",
    "\n",
    "addresses_for_other_recruitments.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 5, 20 중 하나, 어느 주소가 가장 많은지 확인.\n",
    "address_counts_recruitment = recruitment['address_seq1'].value_counts()\n",
    "address_counts_recruitment.loc[[3.0, 5.0, 20.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3이 가장 많으므로, 3으로 채움\n",
    "recruitment['address_seq1'] = recruitment['address_seq1'].fillna(3.0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 주소들은 nan값이 훨씬 많으므로, nan을 '정보없음'으로 함.\n",
    "recruitment['address_seq2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitment['address_seq3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recruitment의 check_box_keyword 고유값 계산\n",
    "all_keywords = recruitment['check_box_keyword'].str.split(';').explode().dropna().unique()\n",
    "\n",
    "num_unique_keywords = len(all_keywords)\n",
    "num_unique_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recruitment 데이터프레임에서 text_keyword가 있으면 1, 아니면 0\n",
    "recruitment['has_text_keyword'] = recruitment['text_keyword'].notna().astype(int)\n",
    "\n",
    "# text_keyword에 해당 키워드가 있으면 1, 아니면 0\n",
    "recruitment['part_time'] = recruitment['text_keyword'].str.contains('아르바이트').fillna(0).astype(int)\n",
    "recruitment['intern'] = recruitment['text_keyword'].str.contains('인턴').fillna(0).astype(int)\n",
    "recruitment['entry_level'] = recruitment['text_keyword'].str.contains('신입').fillna(0).astype(int)\n",
    "recruitment['experienced'] = recruitment['text_keyword'].str.contains('경력|경력직').fillna(0).astype(int)\n",
    "recruitment['team_leader'] = recruitment['text_keyword'].str.contains('팀장|팀장급').fillna(0).astype(int)\n",
    "\n",
    "# check_box_keyword를 나눈 다음 one_hot encoding\n",
    "check_box_encoded = recruitment['check_box_keyword'].str.get_dummies(sep=';')\n",
    "recruitment = pd.concat([recruitment, check_box_encoded], axis=1)\n",
    "\n",
    "# 원래 열들 드랍\n",
    "recruitment_cleaned = recruitment.drop(columns=['address_seq2', 'address_seq3', 'text_keyword', 'check_box_keyword'])\n",
    "recruitment_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company와 recruitment 병합 전 company데이터 살펴보기.\n",
    "unique_company_type_seq = company['company_type_seq'].unique()\n",
    "unique_supply_kind = company['supply_kind'].unique()\n",
    "unique_employee = company['employee'].unique()\n",
    "\n",
    "unique_company_type_seq, unique_supply_kind, unique_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee 최소, 최대\n",
    "min_employee = company['employee'].min()\n",
    "max_employee = company['employee'].max()\n",
    "\n",
    "min_employee, max_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company 정보와 최종 병합. 없는 데이터는 일단 nan으로.\n",
    "final_recruitment = recruitment_cleaned.merge(company, on='recruitment_seq', how='left')\n",
    "final_recruitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "employee nan값 채우기\n",
    "'정보없음' : 0 - nan값인 경우\n",
    "'영세기업' : 1 - employee가 5인 미만인 경우\n",
    "'중소기업' : 2 - employee가 1000명 미만인 경우\n",
    "'중견기업' : 3 - 'employee가 1000명 이상인 경우\n",
    "다른 열도 nan값 채우기, '정보없음'이라는 의미로 0\n",
    "'''\n",
    "final_recruitment['employee_category'] = np.where(final_recruitment['employee'].isna(), 0,\n",
    "                                         np.where(final_recruitment['employee'] < 5, 1,\n",
    "                                                  np.where(final_recruitment['employee'] < 1000, 2, 3)))\n",
    "\n",
    "final_recruitment = final_recruitment.drop(columns=['employee'])\n",
    "final_recruitment[['company_type_seq', 'supply_kind']] = final_recruitment[['company_type_seq', 'supply_kind']].fillna(0)\n",
    "final_recruitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_code_seq2와 job_code_seq3의 유의값, nan값 개수 다시 살펴보기\n",
    "job_code_seq2_nan_count = final_resume['job_code_seq2'].isna().sum()\n",
    "job_code_seq2_non_nan_count = final_resume['job_code_seq2'].notna().sum()\n",
    "\n",
    "job_code_seq3_nan_count = final_resume['job_code_seq3'].isna().sum()\n",
    "job_code_seq3_non_nan_count = final_resume['job_code_seq3'].notna().sum()\n",
    "\n",
    "job_code_seq2_nan_count, job_code_seq2_non_nan_count, job_code_seq3_nan_count, job_code_seq3_non_nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2, seq3의 nan을 '정보없음'으로 변경.\n",
    "final_resume['job_code_seq2'].fillna('정보없음', inplace=True)\n",
    "final_resume['job_code_seq3'].fillna('정보없음', inplace=True)\n",
    "\n",
    "# 각각 label encoding\n",
    "label_encoders = {}\n",
    "\n",
    "for column in ['job_code_seq1', 'job_code_seq2', 'job_code_seq3']:\n",
    "    le = LabelEncoder()\n",
    "    final_resume[column] = le.fit_transform(final_resume[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "final_resume[['job_code_seq1', 'job_code_seq2', 'job_code_seq3']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language 결측치는 어차피 없는 정보이므로 모두 0으로 통일\n",
    "final_resume[['lang_2', 'lang_3', 'lang_4', 'lang_8', 'lang_9']] = final_resume[['lang_2', 'lang_3', 'lang_4', 'lang_8', 'lang_9']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# career_job_code의 경우에도 nan은 정보없음으로 하고, 나머지 label_encoding\n",
    "final_resume['career_job_code'].fillna('정보없음', inplace=True)\n",
    "\n",
    "le_career = LabelEncoder()\n",
    "\n",
    "final_resume['career_job_code'] = le_career.fit_transform(final_resume['career_job_code'])\n",
    "\n",
    "final_resume['career_job_code'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전공은 univ_major_type으로 대체. univ_major와 univ_sub_major는 drop\n",
    "final_resume = final_resume.drop(['univ_major', 'univ_sub_major'], axis=1)\n",
    "final_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_keyword에서 ;를 기준으로 분리\n",
    "keywords = final_resume['text_keyword'].str.split(';').dropna().tolist()\n",
    "\n",
    "# FastText 학습\n",
    "model = FastText(sentences=keywords, vector_size=100, window=5, min_count=2, workers=4, sg=1, epochs=200)\n",
    "\n",
    "# 임베딩 결과 확인을 위한 샘플 키워드 출력\n",
    "sample_keyword = \"디자이너\"\n",
    "model.wv.most_similar(sample_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집화\n",
    "words = list(model.wv.index_to_key)\n",
    "vectors = [model.wv[word] for word in words]\n",
    "\n",
    "n_clusters = 61 # recruitment check_box_keyword의 고유값과 같은 숫자로 매핑\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=CFG['SEED']).fit(vectors)\n",
    "\n",
    "for cluster_num in range(n_clusters):\n",
    "    words_in_cluster = [words[i] for i, label in enumerate(kmeans.labels_) if label == cluster_num]\n",
    "    print(f\"Cluster {cluster_num+1}: {', '.join(words_in_cluster)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 키워드가 어떤 클러스터에 속하는지 확인하는 함수\n",
    "def get_clusters_for_keywords(keywords_list, model, kmeans):\n",
    "    clusters = [kmeans.predict([model.wv[k]])[0] if k in model.wv.index_to_key else -1 for k in keywords_list]\n",
    "    return clusters\n",
    "\n",
    "# 키워드를 분리하고 각 키워드의 클러스터를 가져옴.\n",
    "# NaN 값이 있을 경우 빈 리스트로 처리.\n",
    "all_clusters = final_resume['text_keyword'].str.split(';').fillna('').apply(lambda x: get_clusters_for_keywords(x, model, kmeans))\n",
    "\n",
    "# 각 클러스터에 대해 final_resume에 새로운 열을 추가.\n",
    "for i in range(n_clusters):\n",
    "    final_resume[f'keyword_cluster_{i+1}'] = all_clusters.apply(lambda clusters: int(i in clusters))\n",
    "\n",
    "# text_keyword 열 삭제\n",
    "final_resume.drop(columns=['text_keyword'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certificate의 맞춤법 교정 및 클러스터링을 시도하려고 했으나, 제대로 클러스터링이 안되는 것 같음. \n",
    "# recruitment의 qualification에 해당되는 지원자만 지원했다고 생각하고, certificate_list는 드랍.\n",
    "# 단 certificate_count는 남겨놓음.\n",
    "\n",
    "final_resume = final_resume.drop('certificate_list', axis=1)\n",
    "final_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 object로 남아있는 부분 정리\n",
    "# 1. resume_edu에서 가져왔던 고등학교 정보들 label_encoding\n",
    "columns_to_encode = ['hischool_special_type', 'hischool_nation', 'hischool_gender']\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    final_resume[col] = le.fit_transform(final_resume[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 정보 변환\n",
    "final_resume['updated_date'] = pd.to_datetime(final_resume['updated_date'])\n",
    "final_resume['reg_date'] = pd.to_datetime(final_resume['reg_date'])\n",
    "\n",
    "final_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graduate_date가 0인 데이터가 많은데, degree로 확인이 불가하므로 그냥 0으로 둠.\n",
    "final_resume[final_resume['graduate_date'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphSAGE 모델 만들기\n",
    "# 1. bi-partite 그래프 형성.\n",
    "\n",
    "# ID 매핑\n",
    "resume_ids = {v: k for k, v in enumerate(final_resume['resume_seq'].unique())}\n",
    "recruitment_ids = {v: k for k, v in enumerate(final_recruitment['recruitment_seq'].unique())}\n",
    "\n",
    "# 엣지 생성\n",
    "edges = apply_train.apply(lambda row: (resume_ids[row['resume_seq']], recruitment_ids[row['recruitment_seq']]), axis=1)\n",
    "# 랜덤 워크 위한 양방향 관계를 추가.\n",
    "edges_reversed = [(dst, src) for src, dst in zip(src_nodes, dst_nodes)]\n",
    "\n",
    "# 소스와 목적지 노드 리스트\n",
    "src_nodes = [edge[0] for edge in edges]\n",
    "dst_nodes = [edge[1] for edge in edges]\n",
    "\n",
    "# 소스와 목적지 노드 리스트 (역방향)\n",
    "src_nodes_reversed = [edge[0] for edge in edges_reversed]\n",
    "dst_nodes_reversed = [edge[1] for edge in edges_reversed]\n",
    "\n",
    "# 이분 그래프 생성\n",
    "graph_data = {\n",
    "    ('resume', 'applies_to', 'recruitment'): (torch.tensor(src_nodes), torch.tensor(dst_nodes)),\n",
    "    ('recruitment', 'is_applied_by', 'resume'): (torch.tensor(src_nodes_reversed), torch.tensor(dst_nodes_reversed))\n",
    "}\n",
    "G = dgl.heterograph(graph_data)\n",
    "\n",
    "# 특성 스케일링\n",
    "scaler_resume = MinMaxScaler()\n",
    "scaler_recruitment = MinMaxScaler()\n",
    "\n",
    "resume_features_scaled = scaler_resume.fit_transform(final_resume.drop(columns=['resume_seq']).values)\n",
    "recruitment_features_scaled = scaler_recruitment.fit_transform(final_recruitment.drop(columns=['recruitment_seq']).values)\n",
    "\n",
    "resume_features_tensor = torch.tensor(resume_features_scaled, dtype=torch.float32)\n",
    "recruitment_features_tensor = torch.tensor(recruitment_features_scaled, dtype=torch.float32)\n",
    "\n",
    "# 이분 그래프에 특성 설정\n",
    "G.nodes['resume'].data['features'] = resume_features_tensor\n",
    "G.nodes['recruitment'].data['features'] = recruitment_features_tensor\n",
    "\n",
    "# 그래프 정보 출력\n",
    "num_resumes = G.num_nodes('resume')\n",
    "num_recruitments = G.num_nodes('recruitment')\n",
    "num_edges = G.num_edges('applies_to')\n",
    "\n",
    "print(\"Number of resumes:\", num_resumes)\n",
    "print(\"Number of recruitments:\", num_recruitments)\n",
    "print(\"Number of edges:\", num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemToItemBatchSampler(IterableDataset):\n",
    "    def __init__(self, g, resume_type, recruitment_type, batch_size):\n",
    "        self.g = g\n",
    "        self.resume_type = resume_type\n",
    "        self.recruitment_type = recruitment_type\n",
    "        self.resume_to_recruitment_etype = ('resume', 'applies_to', 'recruitment')\n",
    "        self.recruitment_to_resume_etype = ('recruitment', 'is_applied_by', 'resume')\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            heads = torch.randint(0, self.g.number_of_nodes(self.resume_type), (self.batch_size,))\n",
    "            result = dgl.sampling.random_walk(\n",
    "                self.g,\n",
    "                heads,\n",
    "                metapath=[self.resume_to_recruitment_etype, self.recruitment_to_resume_etype])\n",
    "            tails = result[0][:, 2]\n",
    "            neg_tails = torch.randint(0, self.g.number_of_nodes(self.recruitment_type), (self.batch_size,))\n",
    "            mask = (tails != -1)\n",
    "            yield heads[mask], tails[mask], neg_tails[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling : 코드 참고(https://yamalab.tistory.com/165)\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, resume_type, recruitment_type, random_walk_length, random_walk_restart_prob,\n",
    "                 num_random_walks, num_neighbors, num_layers):\n",
    "        self.g = g\n",
    "        self.resume_type = resume_type\n",
    "        self.recruitment_type = recruitment_type\n",
    "        self.resume_to_recruitment_etype = ('resume', 'applies_to', 'recruitment')\n",
    "        self.recruitment_to_resume_etype = ('recruitment', 'is_applied_by', 'resume')\n",
    "        self.samplers = [\n",
    "            dgl.sampling.PinSAGESampler(g, recruitment_type, resume_type, random_walk_length,\n",
    "                                        random_walk_restart_prob, num_random_walks, num_neighbors)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n",
    "        blocks = []\n",
    "        for sampler in self.samplers:\n",
    "            frontier = sampler(seeds)\n",
    "            block = compact_and_copy(frontier, seeds)\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block) \n",
    "        return blocks\n",
    "\n",
    "    def sample_from_item_pairs(self, heads, tails, neg_tails):\n",
    "        pos_graph = dgl.graph(\n",
    "            (heads, tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.recruitment_type))\n",
    "        neg_graph = dgl.graph(\n",
    "            (heads, neg_tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.recruitment_type))\n",
    "        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n",
    "        return pos_graph, neg_graph, blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinSAGECollator(object):\n",
    "    def __init__(self, sampler, g, ntype):\n",
    "        self.sampler = sampler\n",
    "        self.ntype = ntype\n",
    "        self.g = g\n",
    "\n",
    "    def collate_train(self, batches):\n",
    "        heads, tails, neg_tails = batches[0]\n",
    "        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "        assign_features_to_blocks(blocks, self.g, self.ntype)\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "    def collate_valid(self, batches):\n",
    "        heads, tails = batches[0], batches[1]\n",
    "        pos_graph = dgl.graph((heads, tails), num_nodes=self.g.number_of_nodes(self.ntype))\n",
    "        pos_graph = dgl.compact_graphs([pos_graph])[0]\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "        blocks = self.sampler.sample_blocks(seeds)\n",
    "        assign_features_to_blocks(blocks, self.g, self.ntype)\n",
    "        return pos_graph, blocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
